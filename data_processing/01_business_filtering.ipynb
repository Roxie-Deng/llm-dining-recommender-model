{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_business_filtering.ipynb\n",
    "## Yelp Data Filtering\n",
    "\n",
    "This notebook aims to extract and filter Yelp business data to obtain dining-related businesses located in the top state(s) with the highest concentration of such businesses (i.e., Pennsylvania (PA)).\n",
    "The workflow consists of three main steps:\n",
    "\n",
    "1. **Extract Unique Categories**: Parse Yelp business data to collect all unique business categories.\n",
    "2. **Identify Dining Categories with MiniLM**: Use MiniLM embeddings and cosine similarity to identify dining-related categories based on semantic similarity to seed keywords (e.g., \"restaurant\", \"food\").\n",
    "3. **Filter Top-State Dining Businesses**: Filter businesses that are located in the state(s) with the highest concentration of dining-related businesses, as identified in the previous step.\n",
    "\n",
    "Final Outputs:\n",
    "- `output_categories/unique_categories.json`\n",
    "- `output_categories/dining_semantic_categories.json`\n",
    "- `output_businesses/pa_dining_businesses.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Extract Unique Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500000 records.\n",
      "Extracted 1311 unique categories.\n",
      "Unique categories saved to d:\\Programming\\LLM_RS\\output_categories\\unique_categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Dynamically retrieving path\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Input file for the business dataset\n",
    "business_data_file = os.path.join(BASE_DIR, \"yelp_data\", \"yelp_academic_dataset_business.json\")\n",
    "# Output file for unique categories\n",
    "unique_categories_file = os.path.join(BASE_DIR, \"output_categories\", \"unique_categories.json\")\n",
    "\n",
    "# Define the maximum number of records to process\n",
    "MAX_SCAN_RECORDS = 500000  # Process only the first 500,000 records\n",
    "\n",
    "def extract_unique_categories(file_path, max_records):\n",
    "    \"\"\"Extract all unique categories from the business dataset, limited by max_records.\"\"\"\n",
    "    unique_categories = set()\n",
    "    record_count = 0\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if record_count >= max_records:\n",
    "                break\n",
    "            business = json.loads(line.strip())\n",
    "            if \"categories\" in business and business[\"categories\"]:\n",
    "                categories = [cat.strip() for cat in business[\"categories\"].split(\",\")]\n",
    "                unique_categories.update(categories)\n",
    "            record_count += 1\n",
    "\n",
    "    return unique_categories\n",
    "\n",
    "# Extract unique categories from the business dataset\n",
    "unique_categories = extract_unique_categories(business_data_file, MAX_SCAN_RECORDS)\n",
    "\n",
    "# Save the unique categories to a JSON file\n",
    "with open(unique_categories_file, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(sorted(unique_categories), outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Processed {MAX_SCAN_RECORDS} records.\")\n",
    "print(f\"Extracted {len(unique_categories)} unique categories.\")\n",
    "print(f\"Unique categories saved to {unique_categories_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 53 dining-related categories.\n",
      "Dining-related categories saved to d:\\Programming\\LLM_RS\\output_categories\\dining_semantic_categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Input file for unique categories\n",
    "unique_categories_file = os.path.join(BASE_DIR, \"output_categories\", \"unique_categories.json\")\n",
    "# Output file for dining-related categories\n",
    "dining_categories_file = os.path.join(BASE_DIR, \"output_categories\", \"dining_semantic_categories.json\")\n",
    "\n",
    "# Load unique categories\n",
    "with open(unique_categories_file, 'r', encoding='utf-8') as file:\n",
    "    categories = json.load(file)\n",
    "\n",
    "# Define dining-related seed keywords\n",
    "dining_keywords = [\"restaurant\", \"food\", \"cuisine\", \"bar\", \"dining\"]\n",
    "\n",
    "# Generate embeddings for categories and keywords\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "category_embeddings = model.encode(categories, convert_to_tensor=True)\n",
    "keyword_embeddings = model.encode(dining_keywords, convert_to_tensor=True)\n",
    "\n",
    "# Calculate cosine similarity between keywords and categories\n",
    "similarities = util.cos_sim(keyword_embeddings, category_embeddings)\n",
    "\n",
    "# Extract categories with high similarity to dining keywords\n",
    "threshold = 0.6\n",
    "dining_related_indices = (similarities > threshold).nonzero(as_tuple=True)[1]\n",
    "dining_related = [categories[idx] for idx in dining_related_indices]\n",
    "\n",
    "# Save results\n",
    "with open(dining_categories_file, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(sorted(set(dining_related)), outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Extracted {len(dining_related)} dining-related categories.\")\n",
    "print(f\"Dining-related categories saved to {dining_categories_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Filter All Dining Businesses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Filtered 66852 dining-related businesses from 150346 records.\n",
      "Results saved to d:\\Programming\\LLM_RS\\output_businesses\\dining_related_businesses.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Paths\n",
    "business_data_file = os.path.join(BASE_DIR, \"yelp_data\", \"yelp_academic_dataset_business.json\")\n",
    "dining_categories_file = os.path.join(BASE_DIR, \"output_categories\", \"dining_semantic_categories.json\")\n",
    "dining_businesses_file = os.path.join(BASE_DIR, \"output_businesses\", \"dining_related_businesses.json\")\n",
    "\n",
    "def filter_dining_businesses(business_file, categories_file, output_file):\n",
    "    \"\"\"Filter nationwide dining-related businesses based on pre-selected categories.\"\"\"\n",
    "    with open(categories_file, 'r', encoding='utf-8') as f:\n",
    "        dining_categories = set(json.load(f))\n",
    "\n",
    "    dining_businesses = []\n",
    "    total_records = 0\n",
    "\n",
    "    with open(business_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            total_records += 1\n",
    "            business = json.loads(line.strip())\n",
    "            if \"categories\" in business and business[\"categories\"]:\n",
    "                business_cats = set(cat.strip() for cat in business[\"categories\"].split(\",\"))\n",
    "                if dining_categories & business_cats:\n",
    "                    dining_businesses.append(business)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dining_businesses, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[Step 1] Filtered {len(dining_businesses)} dining-related businesses from {total_records} records.\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "filter_dining_businesses(business_data_file, dining_categories_file, dining_businesses_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2] State-wise distribution of dining businesses:\n",
      "PA: 15842\n",
      "FL: 11274\n",
      "TN: 5473\n",
      "MO: 5287\n",
      "IN: 5274\n",
      "LA: 4851\n",
      "NJ: 4177\n",
      "AZ: 3460\n",
      "AB: 3056\n",
      "NV: 2398\n",
      "ID: 1748\n",
      "CA: 1650\n",
      "IL: 1196\n",
      "DE: 1161\n",
      "NC: 1\n",
      "CO: 1\n",
      "HI: 1\n",
      "MT: 1\n",
      "XMS: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dining_businesses_file = os.path.join(BASE_DIR, \"output_businesses\", \"dining_related_businesses.json\")\n",
    "\n",
    "def count_businesses_by_state(input_file):\n",
    "    \"\"\"Count dining businesses by state.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        businesses = json.load(f)\n",
    "\n",
    "    state_counts = Counter(business.get('state', 'Unknown') for business in businesses)\n",
    "    sorted_states = state_counts.most_common()\n",
    "\n",
    "    print(\"\\n[Step 2] State-wise distribution of dining businesses:\")\n",
    "    for state, count in sorted_states:\n",
    "        print(f\"{state}: {count}\")\n",
    "\n",
    "    return state_counts\n",
    "\n",
    "state_counts = count_businesses_by_state(dining_businesses_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3] Filtered 15842 PA dining-related businesses.\n",
      "Results saved to d:\\Programming\\LLM_RS\\output_businesses\\pa_dining_businesses.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dining_businesses_file = os.path.join(BASE_DIR, \"output_businesses\", \"dining_related_businesses.json\")\n",
    "pa_businesses_file = os.path.join(BASE_DIR, \"output_businesses\", \"pa_dining_businesses.json\")\n",
    "\n",
    "def filter_pa_dining_businesses(input_file, output_file):\n",
    "    \"\"\"Filter PA (Pennsylvania) dining-related businesses from the nationwide dining data.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        businesses = json.load(f)\n",
    "\n",
    "    pa_businesses = [b for b in businesses if b.get('state') == 'PA']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pa_businesses, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n[Step 3] Filtered {len(pa_businesses)} PA dining-related businesses.\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "filter_pa_dining_businesses(dining_businesses_file, pa_businesses_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
