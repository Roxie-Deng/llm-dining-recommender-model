{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_attribute_taxonomy.ipynb\n",
    "## Data Preprocessing 2: Analyze and Simplify \"categories\" and \"attributes\" (dataset_business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all unique categories from pa_filtered_dining_businesses.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique categories extracted: 484\n",
      "Extracted categories saved to: d:\\Programming\\LLM_RS\\output_categories\\pa_unique_categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_businesses\", \"pa_filtered_dining_businesses.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_unique_categories.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load filtered dining businesses\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    businesses = json.load(f)\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = set()\n",
    "\n",
    "for business in businesses:\n",
    "    categories = business.get(\"categories\", \"\")\n",
    "    if categories:\n",
    "        unique_categories.update(map(str.strip, categories.split(',')))\n",
    "\n",
    "# Convert to sorted list\n",
    "unique_categories = sorted(unique_categories)\n",
    "\n",
    "# Save unique categories to file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_categories, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total unique categories extracted: {len(unique_categories)}\")\n",
    "print(f\"Extracted categories saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 484 categories, using a one-hot encoding or multi-hot vector approach will result in very high-dimensional sparse vectors, making computations inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove: irrelevent categories & overly broad terms\n",
    "2. Merge: Synonyms & Redundant Entries (e.g., \"American (New)\" and \"American (Traditional)\" â†’ \"American\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial unique categories: 484\n",
      "Initial unique categories: 484\n",
      "Number of removed categories: 276\n",
      "Final cleaned categories count: 208\n",
      "Extracted categories saved to: d:\\Programming\\LLM_RS\\output_categories\\pa_cleaned_categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_unique_categories.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_cleaned_categories.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load unique categories\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    unique_categories = json.load(f)\n",
    "\n",
    "initial_count = len(unique_categories)\n",
    "print(f\"Initial unique categories: {initial_count}\")\n",
    "\n",
    "# Define categories to remove (Non-food-related)\n",
    "irrelevant_categories = {\n",
    "    \"Accessories\", \"Accountants\", \"Active Life\", \"Acupuncture\", \"Adult\", \"Adult Education\", \"Adult Entertainment\",\n",
    "    \"Airlines\", \"Airport Lounges\", \"Airports\", \"Amusement Parks\", \"Antiques\", \"Apartments\", \"Appliances\",\n",
    "    \"Appliances & Repair\", \"Arcades\", \"Art Classes\", \"Art Galleries\", \"Art Museums\", \"Art Schools\", \"Arts & Crafts\",\n",
    "    \"Arts & Entertainment\", \"Attraction Farms\", \"Auto Glass Services\", \"Auto Repair\", \"Automotive\", \"Axe Throwing\",\n",
    "    \"Baby Gear & Furniture\", \"Banks & Credit Unions\", \"Barbers\", \"Barre Classes\", \"Bartenders\", \"Bartending Schools\",\n",
    "    \"Beaches\", \"Beauty & Spas\", \"Bed & Breakfast\", \"Bike Rentals\", \"Bike Repair/Maintenance\", \"Bike Tours\", \"Bikes\",\n",
    "    \"Boat Charters\", \"Boat Tours\", \"Boating\", \"Body Contouring\", \"Books\", \"Bookstores\", \"Botanical Gardens\",\n",
    "    \"Bowling\", \"Bridal\", \"Business Consulting\", \"Candle Stores\", \"Cannabis Clinics\", \"Cannabis Dispensaries\",\n",
    "    \"Cardio Classes\", \"Cards & Stationery\", \"Casinos\", \"Check Cashing/Pay-day Loans\", \"Children's Clothing\",\n",
    "    \"Christmas Trees\", \"Cinema\", \"Colleges & Universities\", \"Colonics\", \"Comedy Clubs\", \"Comic Books\",\n",
    "    \"Community Service/Non-Profit\", \"Convenience Stores\", \"Cooking Classes\", \"Cooking Schools\",\n",
    "    \"Cosmetics & Beauty Supply\", \"Costumes\", \"Country Clubs\", \"Couriers & Delivery Services\", \"Cultural Center\",\n",
    "    \"Dance Clubs\", \"Day Camps\", \"Day Spas\", \"Dentists\", \"Department Stores\", \"Dinner Theater\", \"Discount Store\",\n",
    "    \"Distilleries\", \"Doctors\", \"Drugstores\", \"Dry Cleaning\", \"Dry Cleaning & Laundry\", \"Education\",\n",
    "    \"Educational Services\", \"Electronics\", \"Event Planning & Services\", \"Eyewear & Opticians\", \"Farmers Market\",\n",
    "    \"Fashion\", \"Festivals\", \"Financial Advising\", \"Financial Services\", \"Fireplace Services\", \"Fitness & Instruction\",\n",
    "    \"Fitness/Exercise Equipment\", \"Flea Markets\", \"Floral Designers\", \"Florists\", \"Flowers & Gifts\", \"Furniture Stores\",\n",
    "    \"Gas Stations\", \"General Dentistry\", \"Gift Shops\", \"Golf\", \"Golf Lessons\", \"Gyms\", \"Hair Salons\", \"Hair Stylists\",\n",
    "    \"Head Shops\", \"Health & Medical\", \"Health Markets\", \"Herbal Shops\", \"Herbs & Spices\", \"Hiking\",\n",
    "    \"Historical Tours\", \"Hobby Shops\", \"Home & Garden\", \"Home Decor\", \"Home Services\", \"Hotels\", \"Hotels & Travel\",\n",
    "    \"Indoor Playcentre\", \"Internet Cafes\", \"Jewelry\", \"Kids Activities\", \"Kitchen & Bath\", \"Kitchen Supplies\",\n",
    "    \"LAN Centers\", \"Landmarks & Historical Buildings\", \"Laundromat\", \"Laundry Services\", \"Lawyers\", \"Libraries\",\n",
    "    \"Local Flavor\", \"Local Services\", \"Macarons\", \"Mags\", \"Marketing\", \"Mass Media\", \"Massage\", \"Massage Therapy\",\n",
    "    \"Medical Spas\", \"Men's Clothing\", \"Mini Golf\", \"Mountain Biking\", \"Museums\", \"Music & DVDs\", \"Music & Video\",\n",
    "    \"Music Venues\", \"Nail Salons\", \"Nail Technicians\", \"Naturopathic/Holistic\", \"Newspapers & Magazines\",\n",
    "    \"Nightlife\", \"Nurseries & Gardening\", \"Nutritionists\", \"Office Equipment\", \"Olive Oil\", \"Organic Stores\",\n",
    "    \"Outdoor Furniture Stores\", \"Outlet Stores\", \"Paint & Sip\", \"Parenting Classes\", \"Parks\",\n",
    "    \"Party & Event Planning\", \"Party Equipment Rentals\", \"Party Supplies\", \"Performing Arts\", \"Personal Chefs\",\n",
    "    \"Personal Shopping\", \"Pet Adoption\", \"Pet Groomers\", \"Pet Services\", \"Pet Stores\", \"Pet Training\", \"Pets\",\n",
    "    \"Photography Stores & Services\", \"Physical Therapy\", \"Pilates\", \"Playgrounds\", \"Pool & Billiards\", \"Pool Halls\",\n",
    "    \"Print Media\", \"Private Tutors\", \"Professional Services\", \"Public Markets\", \"Public Services & Government\",\n",
    "    \"Real Estate\", \"Recreation Centers\", \"Recycling Center\", \"Reflexology\", \"Religious Organizations\", \"Resorts\",\n",
    "    \"Rest Stops\", \"Retail\", \"Salon\", \"Saunas\", \"Shopping\", \"Shopping Centers\", \"Shoe Stores\", \"Skating Rinks\",\n",
    "    \"Skin Care\", \"Social Clubs\", \"Souvenir Shops\", \"Special Education\", \"Specialty Schools\", \"Spiritual Shop\",\n",
    "    \"Sporting Goods\", \"Sports Clubs\", \"Sports Wear\", \"Strip Clubs\", \"Summer Camps\", \"Swimming Pools\",\n",
    "    \"Tabletop Games\", \"Tableware\", \"Tasting Classes\", \"Tea Rooms\", \"Team Building Activities\", \"Tennis\",\n",
    "    \"Thrift Stores\", \"Ticket Sales\", \"Tiki Bars\", \"Tires\", \"Tobacco Shops\", \"Tours\", \"Toy Stores\",\n",
    "    \"Traditional Chinese Medicine\", \"Trainers\", \"Transportation\", \"Travel Services\", \"Tutoring Centers\",\n",
    "    \"Used\", \"Vape Shops\", \"Venues & Event Spaces\", \"Veterinarians\", \"Videos & Video Game Rental\",\n",
    "    \"Vintage & Consignment\", \"Vinyl Records\", \"Visitor Centers\", \"Vitamins & Supplements\", \"Walking Tours\",\n",
    "    \"Watches\", \"Wedding Planning\", \"Weight Loss Centers\", \"Wheel & Rim Repair\", \"Wholesale Stores\", \"Wholesalers\",\n",
    "    \"Wigs\", \"Women's Clothing\", \"Yoga\", \"Zoos\"\n",
    "}\n",
    "\n",
    "# Define overly broad food categories to remove\n",
    "overly_broad_categories = {\n",
    "    \"Food\", \"Restaurants\", \"Grocery\", \"Specialty Food\", \"Ethnic Food\"\n",
    "}\n",
    "\n",
    "# Define category merges (synonyms and redundant labels)\n",
    "category_merges = {\n",
    "    \"American (New)\": \"American\",\n",
    "    \"American (Traditional)\": \"American\",\n",
    "    \"Bubble Tea\": \"Tea-Based Drinks\",\n",
    "    \"Tea Rooms\": \"Tea-Based Drinks\",\n",
    "    \"Steakhouses\": \"Steak\",\n",
    "    \"BBQ\": \"Barbecue\",\n",
    "    \"Fast Food\": \"Quick Service\",\n",
    "    \"Food Trucks\": \"Quick Service\",\n",
    "    \"Bakeries\": \"Desserts\",\n",
    "    \"Ice Cream & Frozen Yogurt\": \"Desserts\",\n",
    "    \"Poke\": \"Hawaiian\",\n",
    "    \"Tex-Mex\": \"Mexican\",\n",
    "    \"Gastropubs\": \"Bars\",\n",
    "    \"Brewpubs\": \"Bars\",\n",
    "    \"Cocktail Bars\": \"Bars\",\n",
    "    \"Wine Bars\": \"Bars\",\n",
    "    \"Juice Bars & Smoothies\": \"Healthy Drinks\",\n",
    "    \"Health Markets\": \"Healthy Drinks\",\n",
    "    \"Vegetarian\": \"Vegan\",\n",
    "    \"Soup\": \"Comfort Food\",\n",
    "    \"Salad\": \"Healthy Food\",\n",
    "    \"Pizza\": \"Italian\",\n",
    "    \"Dim Sum\": \"Chinese\",\n",
    "    \"Noodles\": \"Chinese\",\n",
    "    \"Shanghainese\": \"Chinese\",\n",
    "    \"Szechuan\": \"Chinese\",\n",
    "    \"Cantonese\": \"Chinese\",\n",
    "    \"Tapas Bars\": \"Spanish\",\n",
    "    \"Tapas/Small Plates\": \"Spanish\"\n",
    "}\n",
    "\n",
    "# Process categories\n",
    "processed_categories = {\n",
    "    category_merges.get(c, c) for c in unique_categories \n",
    "    if c not in irrelevant_categories and c not in overly_broad_categories\n",
    "}\n",
    "\n",
    "# Save cleaned categories\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted(processed_categories), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print final stats\n",
    "final_count = len(processed_categories)\n",
    "removed_count = initial_count - final_count\n",
    "print(f\"Initial unique categories: {initial_count}\")\n",
    "print(f\"Number of removed categories: {removed_count}\")\n",
    "print(f\"Final cleaned categories count: {final_count}\")\n",
    "print(f\"Extracted categories saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remain some catregories to be removed\n",
    "\n",
    "1. Automatically match keywords and merge similar categories (e.g., \"Bars\", \"Gay Bars\", \"Beer Bar\" â†’ \"Bars\").\n",
    "2. Remove remaining irrelevant categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial categories count: 208\n",
      "Number of removed categories: 41\n",
      "Final categories count: 167\n",
      "Final cleaned categories saved to: d:\\Programming\\LLM_RS\\output_categories\\pa_refined_categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_cleaned_categories.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_refined_categories.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load cleaned categories\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    cleaned_categories = json.load(f)\n",
    "\n",
    "initial_count = len(cleaned_categories)\n",
    "\n",
    "# Categories to remove (still irrelevant)\n",
    "irrelevant_categories = {\n",
    "    \"Unofficial Yelp Events\", \"Yelp Events\", \"Pumpkin Patches\", \"Guest Houses\", \"Preschools\",\n",
    "    \"Restaurant Supplies\", \"Pharmacy\", \"Hospitals\", \"Hostels\", \"Pick Your Own Farms\"\n",
    "}\n",
    "\n",
    "# Auto-merging rules: Keywords -> Merged category\n",
    "keyword_merges = {\n",
    "    \"bar\": \"Bars\",\n",
    "    \"beer\": \"Bars\",\n",
    "    \"wine\": \"Wine & Spirits\",\n",
    "    \"coffee\": \"Coffee\",\n",
    "    \"tea\": \"Tea-Based Drinks\",\n",
    "    \"juice\": \"Healthy Drinks\",\n",
    "    \"smoothie\": \"Healthy Drinks\",\n",
    "    \"cocktail\": \"Bars\",\n",
    "    \"pub\": \"Bars\",\n",
    "    \"brew\": \"Bars\",\n",
    "    \"poke\": \"Hawaiian\",\n",
    "    \"sushi\": \"Japanese\",\n",
    "    \"ramen\": \"Japanese\",\n",
    "    \"noodle\": \"Asian Fusion\",\n",
    "    \"pasta\": \"Italian\",\n",
    "    \"pizza\": \"Italian\",\n",
    "    \"steak\": \"Steak\",\n",
    "    \"bbq\": \"Barbecue\",\n",
    "    \"dim sum\": \"Chinese\",\n",
    "    \"tapas\": \"Spanish\",\n",
    "    \"sandwich\": \"Delis\",\n",
    "    \"donut\": \"Desserts\",\n",
    "    \"bakery\": \"Desserts\",\n",
    "    \"gelato\": \"Desserts\",\n",
    "    \"ice cream\": \"Desserts\",\n",
    "    \"fast food\": \"Quick Service\",\n",
    "    \"food truck\": \"Quick Service\",\n",
    "    \"vegan\": \"Vegan & Vegetarian\",\n",
    "    \"vegetarian\": \"Vegan & Vegetarian\",\n",
    "}\n",
    "\n",
    "# Process categories\n",
    "processed_categories = set()\n",
    "\n",
    "for category in cleaned_categories:\n",
    "    if category in irrelevant_categories:\n",
    "        continue\n",
    "    \n",
    "    # Apply keyword merging\n",
    "    merged_category = None\n",
    "    for keyword, target in keyword_merges.items():\n",
    "        if keyword in category.lower():\n",
    "            merged_category = target\n",
    "            break\n",
    "    \n",
    "    processed_categories.add(merged_category if merged_category else category)\n",
    "\n",
    "# Convert to sorted list\n",
    "final_categories = sorted(processed_categories)\n",
    "\n",
    "# Save final cleaned categories\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_categories, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print final stats\n",
    "final_count = len(final_categories)\n",
    "removed_count = initial_count - final_count\n",
    "\n",
    "print(f\"Initial categories count: {initial_count}\")\n",
    "print(f\"Number of removed categories: {removed_count}\")\n",
    "print(f\"Final categories count: {final_count}\")\n",
    "print(f\"Final cleaned categories saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maps them into structured groups (Cuisine, Food Type, Service Type, Dietary, etc.).\n",
    "2. Removes irrelevant categories.\n",
    "3. Outputs a cleaned category list in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured categories saved to: d:\\Programming\\LLM_RS\\output_categories\\pa_structured_categories.json\n",
      "Total categorized items: 446\n",
      "Cuisine Types: 23 categories\n",
      "Food Types: 12 categories\n",
      "Service Types: 5 categories\n",
      "Dietary Preferences: 4 categories\n",
      "Other: 402 categories\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_cleaned_categories.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_structured_categories.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load cleaned categories\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    cleaned_categories = json.load(f)\n",
    "\n",
    "# Define structured category groups\n",
    "cuisine_types = {\n",
    "    \"Chinese\", \"Japanese\", \"Mexican\", \"Italian\", \"Thai\", \"Indian\", \"Mediterranean\", \"French\",\n",
    "    \"Vietnamese\", \"Greek\", \"Korean\", \"Brazilian\", \"Turkish\", \"Lebanese\", \"Ethiopian\", \"Spanish\",\n",
    "    \"German\", \"American\", \"African\", \"Filipino\", \"Caribbean\", \"Middle Eastern\", \"Persian/Iranian\"\n",
    "}\n",
    "\n",
    "food_types = {\n",
    "    \"Pizza\", \"Burgers\", \"Sushi\", \"Hot Dogs\", \"Poke\", \"Barbecue\", \"Steak\", \"Tacos\", \"Seafood\",\n",
    "    \"Quick Service\", \"Dim Sum\", \"Breakfast & Brunch\", \"Desserts\", \"Soup\",\n",
    "    \"Salad\", \"Sandwiches\", \"Cafes\", \"Bakeries\", \"Healthy Drinks\", \"Donuts\"\n",
    "}\n",
    "\n",
    "service_types = {\n",
    "    \"Quick Service\", \"Buffets\", \"Fine Dining\", \"Food Trucks\", \"Cafes\", \"Casual Dining\",\n",
    "    \"Bars\", \"Lounges\", \"Wine Bars\", \"Cocktail Bars\", \"Coffee & Tea\", \"Tea-Based Drinks\"\n",
    "}\n",
    "\n",
    "dietary_preferences = {\n",
    "    \"Vegan\", \"Vegetarian\", \"Gluten-Free\", \"Halal\", \"Kosher\"\n",
    "}\n",
    "\n",
    "# Initialize structured categories\n",
    "structured_categories = {\n",
    "    \"Cuisine Types\": [],\n",
    "    \"Food Types\": [],\n",
    "    \"Service Types\": [],\n",
    "    \"Dietary Preferences\": [],\n",
    "    \"Other\": []\n",
    "}\n",
    "\n",
    "# Categorize each cleaned category\n",
    "for category in cleaned_categories:\n",
    "    if category in cuisine_types:\n",
    "        structured_categories[\"Cuisine Types\"].append(category)\n",
    "    elif category in food_types:\n",
    "        structured_categories[\"Food Types\"].append(category)\n",
    "    elif category in service_types:\n",
    "        structured_categories[\"Service Types\"].append(category)\n",
    "    elif category in dietary_preferences:\n",
    "        structured_categories[\"Dietary Preferences\"].append(category)\n",
    "    else:\n",
    "        structured_categories[\"Other\"].append(category)  # Unclassified categories\n",
    "\n",
    "# Save structured categories to file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(structured_categories, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"Structured categories saved to: {output_path}\")\n",
    "print(f\"Total categorized items: {sum(len(v) for v in structured_categories.values())}\")\n",
    "\n",
    "# Show counts for each category type\n",
    "for category_type, items in structured_categories.items():\n",
    "    print(f\"{category_type}: {len(items)} categories\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2: word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories encoded: 484\n",
      "Category embeddings saved to: d:\\Programming\\LLM_RS\\output_categories\\pa_category_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_unique_categories.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_categories\", \"pa_category_embeddings.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load unique categories\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    unique_categories = json.load(f)\n",
    "\n",
    "# Load Sentence-BERT model (MiniLM)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Outputs 384-dimensional vectors\n",
    "\n",
    "# Generate embeddings for each category\n",
    "category_embeddings = {category: model.encode(category).tolist() for category in unique_categories}\n",
    "\n",
    "# Save embeddings to file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(category_embeddings, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total categories encoded: {len(category_embeddings)}\")\n",
    "print(f\"Category embeddings saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Business Attributes by Frequency:\n",
      "RestaurantsTakeOut: 6734 businesses\n",
      "BusinessAcceptsCreditCards: 6682 businesses\n",
      "BusinessParking: 6678 businesses\n",
      "RestaurantsDelivery: 6348 businesses\n",
      "RestaurantsPriceRange2: 6113 businesses\n",
      "BikeParking: 5752 businesses\n",
      "OutdoorSeating: 5536 businesses\n",
      "HasTV: 5307 businesses\n",
      "WiFi: 5288 businesses\n",
      "Ambience: 5261 businesses\n",
      "RestaurantsReservations: 5209 businesses\n",
      "Alcohol: 5123 businesses\n",
      "Caters: 5087 businesses\n",
      "RestaurantsGoodForGroups: 4905 businesses\n",
      "GoodForKids: 4782 businesses\n",
      "NoiseLevel: 4544 businesses\n",
      "RestaurantsAttire: 4455 businesses\n",
      "GoodForMeal: 4062 businesses\n",
      "RestaurantsTableService: 2781 businesses\n",
      "WheelchairAccessible: 2378 businesses\n",
      "\n",
      "Extracted attributes saved to: d:\\Programming\\LLM_RS\\output_attributes\\attribute_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_businesses\", \"pa_filtered_dining_businesses.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_attributes\", \"attribute_analysis.json\")  # Changed path\n",
    "\n",
    "# Load businesses\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    businesses = json.load(f)\n",
    "\n",
    "# Extract attributes\n",
    "attribute_counts = Counter()\n",
    "attribute_values = defaultdict(set)\n",
    "\n",
    "for business in businesses:\n",
    "    attributes = business.get('attributes', {})  # Ensure attributes is a dictionary\n",
    "    if not isinstance(attributes, dict):  # Handle cases where attributes is None or another type\n",
    "        continue\n",
    "\n",
    "    for attr, value in attributes.items():\n",
    "        attribute_counts[attr] += 1  # Count occurrences\n",
    "        attribute_values[attr].add(value)  # Track unique values\n",
    "\n",
    "# Sort attributes by frequency\n",
    "sorted_attributes = sorted(attribute_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top attributes\n",
    "print(\"Top Business Attributes by Frequency:\")\n",
    "for attr, count in sorted_attributes[:20]:  # Show top 20 attributes\n",
    "    print(f\"{attr}: {count} businesses\")\n",
    "\n",
    "# Save the extracted attributes and values for deeper analysis\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"counts\": attribute_counts, \"values\": {k: list(v) for k, v in attribute_values.items()}}, f, indent=2)\n",
    "\n",
    "print(f\"\\nExtracted attributes saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensions:\n",
    "\n",
    "Remove low-occurrence attributes (appear in <5% of businesses).\n",
    "\n",
    "Group categories (e.g., some might be similar).\n",
    "\n",
    "Normalize numerical values (e.g., review_count â†’ log scale transformation).\n",
    "\n",
    "Keep only decision-making features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final cleaned attributes saved to: d:\\Programming\\LLM_RS\\output_attributes\\flattened_attribute.json\n",
      "Total unique attributes after final cleaning: 73\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set base directory and file paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "input_path = os.path.join(BASE_DIR, \"output_businesses\", \"pa_filtered_dining_businesses.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"output_attributes\", \"flattened_attribute.json\")\n",
    "\n",
    "# Load businesses\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    businesses = json.load(f)\n",
    "\n",
    "# Function to parse and normalize attribute values\n",
    "def parse_value(value):\n",
    "    \"\"\"Convert string representations of dicts to actual dicts & normalize other values.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed_value = ast.literal_eval(value)  # Convert string to dictionary\n",
    "            if isinstance(parsed_value, dict):\n",
    "                return parsed_value\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Not a dictionary, keep as is\n",
    "    return value  # Return raw value if not a dictionary\n",
    "\n",
    "# Flattened attributes storage\n",
    "flattened_attributes = defaultdict(set)\n",
    "\n",
    "for business in businesses:\n",
    "    attributes = business.get(\"attributes\", {})\n",
    "    if not isinstance(attributes, dict):  # Skip invalid attributes\n",
    "        continue\n",
    "\n",
    "    for attr, value in attributes.items():\n",
    "        parsed_value = parse_value(value)  # Try to parse the value\n",
    "        \n",
    "        if isinstance(parsed_value, dict):  # Handle nested dictionaries\n",
    "            for sub_attr, sub_value in parsed_value.items():\n",
    "                if sub_value is None:\n",
    "                    sub_value = \"Unknown\"  # Replace None with 'Unknown'\n",
    "                elif isinstance(sub_value, bool):\n",
    "                    sub_value = \"True\" if sub_value else \"False\"\n",
    "                flattened_attributes[f\"{attr}_{sub_attr}\"].add(str(sub_value))\n",
    "        else:\n",
    "            # Normalize boolean and categorical values\n",
    "            if parsed_value is None or parsed_value == \"None\":\n",
    "                parsed_value = \"Unknown\"\n",
    "            elif isinstance(parsed_value, bool):\n",
    "                parsed_value = \"True\" if parsed_value else \"False\"\n",
    "            elif isinstance(parsed_value, str):\n",
    "                parsed_value = parsed_value.strip(\"u'\")  # Remove Unicode markers\n",
    "            \n",
    "            flattened_attributes[attr].add(str(parsed_value))\n",
    "\n",
    "# Convert sets to lists for JSON saving\n",
    "cleaned_attributes = {k: list(v) for k, v in flattened_attributes.items() if v != {\"Unknown\"}}  # Remove all-\"Unknown\" attributes\n",
    "\n",
    "# Save cleaned and structured attributes\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_attributes, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal cleaned attributes saved to: {output_path}\")\n",
    "print(f\"Total unique attributes after final cleaning: {len(cleaned_attributes)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
